{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the file\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"Eluvio_DS_Challenge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the basic imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up_votes</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Scores killed in Pakistan clashes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Japan resumes refuelling mission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>US presses Egypt on Gaza border</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Jump-start economy: Give health care to all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Council of Europe bashes EU&amp;UN terror blacklist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   up_votes                                            title\n",
       "0         3                Scores killed in Pakistan clashes\n",
       "1         2                 Japan resumes refuelling mission\n",
       "2         3                  US presses Egypt on Gaza border\n",
       "3         1     Jump-start economy: Give health care to all \n",
       "4         4  Council of Europe bashes EU&UN terror blacklist"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing irrelevant attributes\n",
    "\n",
    "df = df.drop(\"category\", axis = 1)\n",
    "df = df.drop(\"down_votes\", axis = 1)\n",
    "df = df.drop(\"time_created\", axis = 1)\n",
    "df = df.drop(\"date_created\", axis = 1)\n",
    "df = df.drop(\"over_18\", axis = 1)\n",
    "df = df.drop(\"author\", axis = 1)\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sahith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sahith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# code for defining the stemmer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To get the stems of words in a sentence.\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "# To get the words themself in a sentence.\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = df.title.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full stems and tokens to build vocabulary\n",
    "def tokenized_stemmed(title):\n",
    "    totalvocab_stemmed = []\n",
    "    totalvocab_tokenized = []\n",
    "    for i in title:\n",
    "        allwords_stemmed = tokenize_and_stem(i) \n",
    "        totalvocab_stemmed.extend(allwords_stemmed) \n",
    "\n",
    "        allwords_tokenized = tokenize_only(i)\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    return totalvocab_stemmed, totalvocab_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "totalvocab_stemmed_, totalvocab_tokenized_ = tokenized_stemmed(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7194609\n"
     ]
    }
   ],
   "source": [
    "print(len(totalvocab_stemmed_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule out repetitions of stem-token pairs\n",
    "totalvocab = zip(totalvocab_stemmed_, totalvocab_tokenized_)\n",
    "totalvocab = list(set(totalvocab))\n",
    "totalvocab_stemmed, totalvocab_tokenized = zip(*totalvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114842\n"
     ]
    }
   ],
   "source": [
    "print(len(totalvocab_stemmed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the stop words set\n",
    "import sklearn.feature_extraction.text as text\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing text using Tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahith\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', \"n't\", 'need', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sha', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(509236, 1814)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tf-idf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df =10**-3 ,analyzer = 'word', max_features=len(set(totalvocab_stemmed)), stop_words=my_stop_words, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(title)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<509236x1814 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3565430 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for applying ML Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "thre = np.quantile(df['up_votes'], 0.8)\n",
    "y = [1 if i > thre else 0 for i in df['up_votes']]\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size = 0.2, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89     81988\n",
      "           1       0.56      0.00      0.00     19860\n",
      "\n",
      "    accuracy                           0.81    101848\n",
      "   macro avg       0.68      0.50      0.45    101848\n",
      "weighted avg       0.76      0.81      0.72    101848\n",
      "\n",
      "0.8050624459979577\n"
     ]
    }
   ],
   "source": [
    "# using Multinomial Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)\n",
    "print(classification_report(y_test, y_predict))\n",
    "print(accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89     81988\n",
      "           1       0.54      0.04      0.07     19860\n",
      "\n",
      "    accuracy                           0.81    101848\n",
      "   macro avg       0.68      0.52      0.48    101848\n",
      "weighted avg       0.76      0.81      0.73    101848\n",
      "\n",
      "0.8061817610556908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahith\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# using LogisticRegression\n",
    "LR = LogisticRegression(C=1.0, penalty='l2', tol=0.01)\n",
    "LR.fit(X_train, y_train)\n",
    "y_predict = LR.predict(X_test)\n",
    "LR.score(X_test, y_test)\n",
    "print(classification_report(y_test, y_predict))\n",
    "print(accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89     81988\n",
      "           1       0.69      0.00      0.01     19860\n",
      "\n",
      "    accuracy                           0.81    101848\n",
      "   macro avg       0.75      0.50      0.45    101848\n",
      "weighted avg       0.78      0.81      0.72    101848\n",
      "\n",
      "0.805317728379546\n"
     ]
    }
   ],
   "source": [
    "# By using Gradient Boosting Algorithm\n",
    "gbdt = GradientBoostingClassifier()\n",
    "gbdt.fit(X_train, y_train)\n",
    "y_predict = gbdt.predict(X_test)\n",
    "gbdt.score(X_test, y_test)\n",
    "print(classification_report(y_test, y_predict))\n",
    "print(accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The title was preprocessed using NLTK built in packages and used Tf-idf vectorizer to vectorize the text. Tf-idf was used because Tf-idf calculates the weight of a particular word rather than relying on number of occurances which other approaches do. Also, Tf-idf comes in handy when dealing with large amount of data as the repetition of words is seen often and relying on just frequency of words doesn't work. \n",
    "\n",
    "Data is split in 80% for training and 20% for testing using sklearn\n",
    "\n",
    "Multinomial Naive Bayes, Logistic Regression and Gradient Boosting algorithms are working pretty well with the data provided. The accuracy of around 8.05 strenghtens our argument that there is a strong relation between the title and the up votes. The up votes are mostly related to the topics that audience are interested in. Also, to get a deeper understanding of the model, I've further trained an Recurrent Neural Network architecture on Google TPU (as I was asked to treat the problem as large-scale and will not fit into my RAM) to exploit the relationship between the titles and up votes. Please review the results and my approach of \"Deep_Learning.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
